{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础GPT的实现\n",
    "\n",
    "本节将手写一个简单的GPT，并使用 torch.rand 生成数据来测试。为了实现模型跑通，以及使模型收敛\n",
    "包含以下内容：\n",
    "\n",
    "- [x] 模型结构\n",
    "- [x] 损失函数\n",
    "- [x] 优化器\n",
    "- [x] 模型收敛验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.模型结构\n",
    "\n",
    "### 1.1 实现单头注意力\n",
    "\n",
    "输入 x：[seq_len, d_model]\n",
    "K Q V线性矩阵：K[d_model, d_k], Q[d_model, d_k], V[d_model, d_v], W[d_model, d_model]\n",
    "\n",
    "输出：y[seq_len, d_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        # super()函数是用于调用父类(超类)的一个方法\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.K = nn.Linear(d_model, d_k)\n",
    "        self.Q = nn.Linear(d_model, d_k)\n",
    "        self.V = nn.Linear(d_model, d_v)\n",
    "        self.W = nn.Linear(d_v, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, d_model]\n",
    "        K = self.K(x)  # [seq_len, d_k]\n",
    "        Q = self.Q(x)  # [seq_len, d_k]\n",
    "        V = self.V(x)  # [seq_len, d_v]\n",
    "\n",
    "        # 计算注意力权重，即Q和K的点积，再除以根号下d_k， K.transpose(-2, -1)表示将K的最后一个维度和倒数第二个维度交换位置,这样可以使得K和Q的点积结果是一个方阵得到 [seq_len, seq_len]的注意力权重矩阵\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(K.shape[-1], dtype=torch.float32)) # [seq_len, seq_len]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        y = torch.matmul(attention_weights, V)  # [seq_len, d_v]\n",
    "        y = self.W(y)  # [seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码分析：\n",
    "1.. `__init__`方法中，我们定义了四个线性层，分别用于计算K、Q、V和W。\n",
    "2. `forward`方法中，我们首先计算K、Q、V，然后计算注意力权重，最后计算注意力输出。\n",
    "3. 注意力权重计算公式为：`attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))`，其中`torch.matmul`表示矩阵乘法，`K.transpose(-2, -1)`表示将K的最后一个维度和倒数第二个维度交换位置，这样可以使得K和Q的点积结果是一个方阵。\n",
    "4. 注意力输出计算公式为：`y = torch.matmul(attention_weights, V)`，其中`torch.matmul`表示矩阵乘法，`attention_weights`表示注意力权重，`V`表示V矩阵。\n",
    "5. 注意力输出经过线性层W后得到最终的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "seq_len = 10\n",
    "x = torch.rand(seq_len, d_model)\n",
    "attention = SelfAttention(d_model, d_k, d_v)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [seq_len, d_model]\n",
    "\n",
    "# 输出 torch.Size([10, 512])\n",
    "# 维度分析 ：x [seq_len, d_model] , Wk [d_model, d_k] , Wq [d_model, d_k] , Wv [d_model, d_v] , W [d_v, d_model]\n",
    "#                                   K [seq_len, d_k] , Q [seq_len, d_k] , V [seq_len, d_v] \n",
    "#                   attention_weights [seq_len, seq_len]\n",
    "#                   y = attention_weights * V [seq_len, d_v] * W [d_v, d_model] \n",
    "#                     = [seq_len, seq_len] * [seq_len, d_v] * [d_v, d_model] = [seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 实现多头注意力（非batch版本）\n",
    "\n",
    "输入 x：[seq_len, d_model]\n",
    "\n",
    "K Q V 线性矩阵：K[d_model,  num_heads * d_k], Q[d_model, num_heads * d_k], V[d_model, num_heads * d_v], W[num_heads * d_v, d_model]\n",
    "\n",
    "输出：y[seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.K = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.Q = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.V = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W = nn.Linear(d_v * num_heads, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, d_model]\n",
    "        K = self.K(x).view(-1, self.num_heads, self.d_k)  # [seq_len, num_heads, d_k]\n",
    "        Q = self.Q(x).view(-1, self.num_heads, self.d_k)  # [seq_len, num_heads, d_k]\n",
    "        V = self.V(x).view(-1, self.num_heads, self.d_v)  # [seq_len, num_heads, d_v]\n",
    "        \n",
    "        # [seq_len, num_heads, d_k] * [seq_len, d_k, num_heads] = [seq_len, num_heads, num_heads]\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) \n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1) # [seq_len, num_heads, num_heads]\n",
    "        \n",
    "        # 计算注意力输出 [seq_len, num_heads, num_heads] * [seq_len, num_heads, d_v] = [seq_len, num_heads, d_v]\n",
    "        y = torch.matmul(attention_weights, V)  # [seq_len, num_heads, d_v]\n",
    "        y = y.view(-1, self.num_heads * self.d_v)  # [seq_len, d_v * num_heads]\n",
    "        \n",
    "        # 计算多头注意力输出\n",
    "        y = self.W(y)  # [seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码分析：\n",
    "1. 区别于单头注意力，多头注意力在计算K、Q、V时，对多头的处理体现在view函数中，将d_model维度拆分为num_heads个d_k和d_v维度。\n",
    "2. 在计算注意力权重时，将K和Q的点积结果除以根号下d_k，以防止点积结果过大。\n",
    "3. 在计算注意力输出时， y 也是先 经过view函数将num_heads和d_v维度合并，再经过线性层W得到最终的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "seq_len = 10\n",
    "num_heads = 8\n",
    "x = torch.rand(seq_len, d_model)\n",
    "attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [seq_len, d_model]\n",
    "\n",
    "# 维度分析 ： x [seq_len, d_model] , Wk [d_model, d_k * num_heads] , Wq [d_model, d_k * num_heads] , Wv [d_model, d_v * num_heads] , W [d_v * num_heads, d_model]\n",
    "#                                   K [seq_len, num_heads, d_k] , Q [seq_len, num_heads, d_k] , V [seq_len, num_heads, d_v]\n",
    "#                    attention_weights [seq_len, num_heads, num_heads]\n",
    "#                 y = attention_weights * V [seq_len, num_heads, d_v]  = [seq_len, num_heads, num_heads] * [seq_len, num_heads, d_v] = [seq_len, num_heads, d_v]\n",
    "#                 y = y.view(-1, self.num_heads * self.d_v) * W = [seq_len, d_v * num_heads] * [d_v * num_heads, d_model] = [seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 实现多头注意力（batch）\n",
    "\n",
    "输入 x：[batch_size, seq_len, d_model]\n",
    "\n",
    "K Q V 线性矩阵：K[d_model,  num_heads * d_k], Q[d_model, num_heads * d_k], V[d_model, num_heads * d_v], W[num_heads * d_v, d_model]\n",
    "\n",
    "输出：y[batch_size, seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.K = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.Q = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.V = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W = nn.Linear(d_v * num_heads, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "\n",
    "        K = self.K(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        Q = self.Q(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        V = self.V(x).view(-1, self.num_heads, self.d_v)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        \n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # [batch_size * seq_len, num_heads, num_heads]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # [batch_size * seq_len, num_heads, num_heads]\n",
    "      \n",
    "    \n",
    "        y = torch.matmul(attention_weights, V)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        y = y.view(-1, self.num_heads * self.d_v)  # [batch_size * seq_len, d_v * num_heads]\n",
    "       \n",
    "        y = self.W(y)  # [batch_size * seq_len, d_model]\n",
    "        y = y.view(-1, x.size(1), self.d_model)  # [batch_size, seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "# 维度分析 ： x [batch_size, seq_len, d_model] , Wk [d_model, d_k * num_heads] , Wq [d_model, d_k * num_heads] , Wv [d_model, d_v * num_heads] , W [d_v * num_heads, d_model]\n",
    "#                                   K [batch_size * seq_len, num_heads, d_k] , Q [batch_size * seq_len, num_heads, d_k] , V [batch_size * seq_len, num_heads, d_v]\n",
    "#                    attention_weights [batch_size * seq_len, num_heads, num_heads]\n",
    "#                    y [batch_size * seq_len, num_heads, d_v]\n",
    "#                    y = y.view(-1, self.num_heads * self.d_v) [batch_size * seq_len, d_v * num_heads]\n",
    "#                    y = self.W(y) [batch_size * seq_len, d_model]\n",
    "#                    y = y.view(-1, x.size(1), self.d_model) [batch_size, seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续优化，添加残差连接和层归一化\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.K = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.Q = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.V = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W = nn.Linear(d_v * num_heads, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        K = self.K(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        Q = self.Q(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        V = self.V(x).view(-1, self.num_heads, self.d_v)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # [batch_size * seq_len, num_heads, num_heads]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # [batch_size * seq_len, num_heads, num_heads]\n",
    "        y = torch.matmul(attention_weights, V)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        y = y.view(-1, self.num_heads * self.d_v)  # [batch_size * seq_len, d_v * num_heads]\n",
    "        y = self.W(y)  # [batch_size * seq_len, d_model]\n",
    "        \n",
    "        y = y.view(-1, x.size(1), self.d_model)  # [batch_size, seq_len, d_model]\n",
    "        y = self.layer_norm(x + y)  # [batch_size, seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 实现 Block\n",
    "Block 在 GPT 中由两个部分组成：一个 MultiHeadAttention 层和一个前馈神经网络层。前馈神经网络层由两个线性层和一个激活函数组成。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads, d_ff):\n",
    "        super(Block, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        y = self.attention(x)\n",
    "\n",
    "        y = self.feed_forward(y)\n",
    "        y = self.layer_norm(x + y)\n",
    "        return y\n",
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "block = Block(d_model, d_k, d_v, num_heads, d_ff)\n",
    "y = block(x)\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 实现 GPT\n",
    "GPT 由多个 Block 组成，每个 Block 包含一个 MultiHeadAttention 层和一个前馈神经网络层。GPT 的输入是一个序列，输出是序列的每个位置的隐藏状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGPT(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads, d_ff, num_layers):\n",
    "\n",
    "        super(FGPT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([Block(d_model, d_k, d_v, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 768\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 12\n",
    "d_ff = 3072\n",
    "num_layers = 12\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "gpt = FGPT(d_model, d_k, d_v, num_heads, d_ff, num_layers)\n",
    "y = gpt(x)\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 收敛测试\n",
    "检查模型是否能正常收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3319876194000244\n",
      "Epoch 10, Loss: 1.3319876194000244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mfgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y)\n\u001b[1;32m     32\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mFGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# x: [batch_size, seq_len, d_model]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 12\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# x: [batch_size, seq_len, d_model]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x)\n\u001b[0;32m---> 16\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x \u001b[38;5;241m+\u001b[39m y)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ddp/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "d_model = 768\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 12\n",
    "d_ff = 3072\n",
    "num_layers = 12\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成任意大批量的数据\n",
    "data_leng = 100\n",
    "x = torch.rand(data_leng, seq_len, d_model)\n",
    "# 生成标签\n",
    "y = torch.rand(data_leng, seq_len, d_model)\n",
    "\n",
    "# 构建模型\n",
    "fgpt = FGPT(d_model, d_k, d_v, num_heads, d_ff, num_layers)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = fgpt(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
