{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础GPT的实现\n",
    "\n",
    "本节将手写一个简单的GPT，并使用 torch.rand 生成数据来测试。为了实现模型跑通，以及使模型收敛\n",
    "包含以下内容：\n",
    "\n",
    "- [x] 模型结构\n",
    "- [x] 损失函数\n",
    "- [x] 优化器\n",
    "- [x] 模型收敛验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.模型结构\n",
    "\n",
    "### 1.1 实现单头注意力\n",
    "\n",
    "输入 x：[seq_len, d_model]\n",
    "K Q V线性矩阵：K[d_model, d_k], Q[d_model, d_k], V[d_model, d_v], W[d_model, d_model]\n",
    "\n",
    "输出：y[seq_len, d_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        # super()函数是用于调用父类(超类)的一个方法\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.K = nn.Linear(d_model, d_k)\n",
    "        self.Q = nn.Linear(d_model, d_k)\n",
    "        self.V = nn.Linear(d_model, d_v)\n",
    "        self.W = nn.Linear(d_v, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, d_model]\n",
    "        K = self.K(x)  # [seq_len, d_k]\n",
    "        Q = self.Q(x)  # [seq_len, d_k]\n",
    "        V = self.V(x)  # [seq_len, d_v]\n",
    "\n",
    "        # 计算注意力权重，即Q和K的点积，再除以根号下d_k， K.transpose(-2, -1)表示将K的最后一个维度和倒数第二个维度交换位置,这样可以使得K和Q的点积结果是一个方阵得到 [seq_len, seq_len]的注意力权重矩阵\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(K.shape[-1], dtype=torch.float32)) # [seq_len, seq_len]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        y = torch.matmul(attention_weights, V)  # [seq_len, d_v]\n",
    "        y = self.W(y)  # [seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码分析：\n",
    "1.. `__init__`方法中，我们定义了四个线性层，分别用于计算K、Q、V和W。\n",
    "2. `forward`方法中，我们首先计算K、Q、V，然后计算注意力权重，最后计算注意力输出。\n",
    "3. 注意力权重计算公式为：`attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))`，其中`torch.matmul`表示矩阵乘法，`K.transpose(-2, -1)`表示将K的最后一个维度和倒数第二个维度交换位置，这样可以使得K和Q的点积结果是一个方阵。\n",
    "4. 注意力输出计算公式为：`y = torch.matmul(attention_weights, V)`，其中`torch.matmul`表示矩阵乘法，`attention_weights`表示注意力权重，`V`表示V矩阵。\n",
    "5. 注意力输出经过线性层W后得到最终的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "seq_len = 10\n",
    "x = torch.rand(seq_len, d_model)\n",
    "attention = SelfAttention(d_model, d_k, d_v)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [seq_len, d_model]\n",
    "\n",
    "# 输出 torch.Size([10, 512])\n",
    "# 维度分析 ：x [seq_len, d_model] , Wk [d_model, d_k] , Wq [d_model, d_k] , Wv [d_model, d_v] , W [d_v, d_model]\n",
    "#                                   K [seq_len, d_k] , Q [seq_len, d_k] , V [seq_len, d_v] \n",
    "#                   attention_weights [seq_len, seq_len]\n",
    "#                   y = attention_weights * V [seq_len, d_v] * W [d_v, d_model] \n",
    "#                     = [seq_len, seq_len] * [seq_len, d_v] * [d_v, d_model] = [seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 实现多头注意力（非batch版本）\n",
    "\n",
    "输入 x：[seq_len, d_model]\n",
    "\n",
    "K Q V 线性矩阵：K[d_model,  num_heads * d_k], Q[d_model, num_heads * d_k], V[d_model, num_heads * d_v], W[num_heads * d_v, d_model]\n",
    "\n",
    "输出：y[seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.K = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.Q = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.V = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W = nn.Linear(d_v * num_heads, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, d_model]\n",
    "        K = self.K(x).view(-1, self.num_heads, self.d_k)  # [seq_len, num_heads, d_k]\n",
    "        Q = self.Q(x).view(-1, self.num_heads, self.d_k)  # [seq_len, num_heads, d_k]\n",
    "        V = self.V(x).view(-1, self.num_heads, self.d_v)  # [seq_len, num_heads, d_v]\n",
    "        \n",
    "        # [seq_len, num_heads, d_k] * [seq_len, d_k, num_heads] = [seq_len, num_heads, num_heads]\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) \n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1) # [seq_len, num_heads, num_heads]\n",
    "        \n",
    "        # 计算注意力输出 [seq_len, num_heads, num_heads] * [seq_len, num_heads, d_v] = [seq_len, num_heads, d_v]\n",
    "        y = torch.matmul(attention_weights, V)  # [seq_len, num_heads, d_v]\n",
    "        y = y.view(-1, self.num_heads * self.d_v)  # [seq_len, d_v * num_heads]\n",
    "        \n",
    "        # 计算多头注意力输出\n",
    "        y = self.W(y)  # [seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码分析：\n",
    "1. 区别于单头注意力，多头注意力在计算K、Q、V时，对多头的处理体现在view函数中，将d_model维度拆分为num_heads个d_k和d_v维度。\n",
    "2. 在计算注意力权重时，将K和Q的点积结果除以根号下d_k，以防止点积结果过大。\n",
    "3. 在计算注意力输出时， y 也是先 经过view函数将num_heads和d_v维度合并，再经过线性层W得到最终的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "seq_len = 10\n",
    "num_heads = 8\n",
    "x = torch.rand(seq_len, d_model)\n",
    "attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [seq_len, d_model]\n",
    "\n",
    "# 维度分析 ： x [seq_len, d_model] , Wk [d_model, d_k * num_heads] , Wq [d_model, d_k * num_heads] , Wv [d_model, d_v * num_heads] , W [d_v * num_heads, d_model]\n",
    "#                                   K [seq_len, num_heads, d_k] , Q [seq_len, num_heads, d_k] , V [seq_len, num_heads, d_v]\n",
    "#                    attention_weights [seq_len, num_heads, num_heads]\n",
    "#                 y = attention_weights * V [seq_len, num_heads, d_v]  = [seq_len, num_heads, num_heads] * [seq_len, num_heads, d_v] = [seq_len, num_heads, d_v]\n",
    "#                 y = y.view(-1, self.num_heads * self.d_v) * W = [seq_len, d_v * num_heads] * [d_v * num_heads, d_model] = [seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 实现多头注意力（batch）\n",
    "\n",
    "输入 x：[batch_size, seq_len, d_model]\n",
    "\n",
    "K Q V 线性矩阵：K[d_model,  num_heads * d_k], Q[d_model, num_heads * d_k], V[d_model, num_heads * d_v], W[num_heads * d_v, d_model]\n",
    "\n",
    "输出：y[batch_size, seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        self.K = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.Q = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.V = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W = nn.Linear(d_v * num_heads, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "\n",
    "        K = self.K(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        Q = self.Q(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        V = self.V(x).view(-1, self.num_heads, self.d_v)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        \n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # [batch_size * seq_len, num_heads, num_heads]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # [batch_size * seq_len, num_heads, num_heads]\n",
    "      \n",
    "    \n",
    "        y = torch.matmul(attention_weights, V)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        y = y.view(-1, self.num_heads * self.d_v)  # [batch_size * seq_len, d_v * num_heads]\n",
    "       \n",
    "        y = self.W(y)  # [batch_size * seq_len, d_model]\n",
    "        y = y.view(-1, x.size(1), self.d_model)  # [batch_size, seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "# 维度分析 ： x [batch_size, seq_len, d_model] , Wk [d_model, d_k * num_heads] , Wq [d_model, d_k * num_heads] , Wv [d_model, d_v * num_heads] , W [d_v * num_heads, d_model]\n",
    "#                                   K [batch_size * seq_len, num_heads, d_k] , Q [batch_size * seq_len, num_heads, d_k] , V [batch_size * seq_len, num_heads, d_v]\n",
    "#                    attention_weights [batch_size * seq_len, num_heads, num_heads]\n",
    "#                    y [batch_size * seq_len, num_heads, d_v]\n",
    "#                    y = y.view(-1, self.num_heads * self.d_v) [batch_size * seq_len, d_v * num_heads]\n",
    "#                    y = self.W(y) [batch_size * seq_len, d_model]\n",
    "#                    y = y.view(-1, x.size(1), self.d_model) [batch_size, seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续优化，添加残差连接和层归一化\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.K = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.Q = nn.Linear(d_model, d_k * num_heads)\n",
    "        self.V = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W = nn.Linear(d_v * num_heads, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        K = self.K(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        Q = self.Q(x).view(-1, self.num_heads, self.d_k)  # [batch_size * seq_len, num_heads, d_k]\n",
    "        V = self.V(x).view(-1, self.num_heads, self.d_v)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))  # [batch_size * seq_len, num_heads, num_heads]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)  # [batch_size * seq_len, num_heads, num_heads]\n",
    "        y = torch.matmul(attention_weights, V)  # [batch_size * seq_len, num_heads, d_v]\n",
    "        y = y.view(-1, self.num_heads * self.d_v)  # [batch_size * seq_len, d_v * num_heads]\n",
    "        y = self.W(y)  # [batch_size * seq_len, d_model]\n",
    "        \n",
    "        y = y.view(-1, x.size(1), self.d_model)  # [batch_size, seq_len, d_model]\n",
    "        y = self.layer_norm(x + y)  # [batch_size, seq_len, d_model]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "y = attention(x)\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 实现 Block\n",
    "Block 在 GPT 中由两个部分组成：一个 MultiHeadAttention 层和一个前馈神经网络层。前馈神经网络层由两个线性层和一个激活函数组成。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads, d_ff):\n",
    "        super(Block, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, d_k, d_v, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        y = self.attention(x)\n",
    "\n",
    "        y = self.feed_forward(y)\n",
    "        y = self.layer_norm(x + y)\n",
    "        return y\n",
    "# 测试\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "block = Block(d_model, d_k, d_v, num_heads, d_ff)\n",
    "y = block(x)\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 实现 GPT\n",
    "GPT 由多个 Block 组成，每个 Block 包含一个 MultiHeadAttention 层和一个前馈神经网络层。GPT 的输入是一个序列，输出是序列的每个位置的隐藏状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGPT(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, num_heads, d_ff, num_layers):\n",
    "\n",
    "        super(FGPT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([Block(d_model, d_k, d_v, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "d_model = 768\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 12\n",
    "d_ff = 3072\n",
    "num_layers = 12\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成一个batch_size * seq_len * d_model 的随机张量\n",
    "x = torch.rand(batch_size, seq_len, d_model)\n",
    "gpt = FGPT(d_model, d_k, d_v, num_heads, d_ff, num_layers)\n",
    "y = gpt(x)\n",
    "\n",
    "# 打印输出张量的形状\n",
    "print(y.shape)  # [batch_size, seq_len, d_model]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 收敛测试\n",
    "检查模型是否能正常收敛\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3334763050079346\n",
      "Epoch 10, Loss: 1.1047908067703247\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "d_model = 768\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "num_heads = 12\n",
    "d_ff = 3072\n",
    "num_layers = 12\n",
    "seq_len = 10\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 生成任意大批量的数据\n",
    "data_leng = 100\n",
    "x = torch.rand(data_leng, seq_len, d_model)\n",
    "# 生成标签\n",
    "y = torch.rand(data_leng, seq_len, d_model)\n",
    "\n",
    "# 构建模型\n",
    "gpt = GPT(d_model, d_k, d_v, num_heads, d_ff, num_layers)\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(gpt.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = gpt(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "###PATH:./FGPT/01 GPT.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
